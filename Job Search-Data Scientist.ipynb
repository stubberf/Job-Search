{
 "metadata": {
  "name": "",
  "signature": "sha256:00cdef9a82f3b3f3dce30551d5cdf411d91aba11037acb2ec6ffbee06751dc62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below attempts to find the most important words to place on a resume/Linkend profile by searching the \"requirement\" section of text for many open positions in a related field. In particular this script is being used to find \"Data Scientist\" positions but in theory could be used for other professions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Search for Jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This is a class used to strip the HTML tags from text\n",
      "#This code was taken from the webpage below and written by That1Guy (Thanks!)\n",
      "#http://stackoverflow.com/questions/17011732/how-can-i-convert-html-into-text-without-markup-in-python\n",
      "from HTMLParser import HTMLParser\n",
      "\n",
      "class MLStripper(HTMLParser):\n",
      "    def __init__(self):\n",
      "        self.reset()\n",
      "        self.fed = []\n",
      "    def handle_data(self, d):\n",
      "        self.fed.append(d)\n",
      "    def get_data(self):\n",
      "        return ''.join(self.fed)\n",
      "\n",
      "def strip_tags(html):\n",
      "    s = MLStripper()\n",
      "    s.feed(html)\n",
      "    return s.get_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import url and xml elementTree librarys\n",
      "import urllib, urllib2\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "\n",
      "#The keyword to send for search of open positions\n",
      "#************************************************\n",
      "kw = 'data scientist'\n",
      "#kw = 'electrical engineer'\n",
      "#************************************************\n",
      "\n",
      "#The keywords to exclude\n",
      "#************************************************\n",
      "ekw = 'medical,laboratory'\n",
      "#************************************************\n",
      "\n",
      "#CareerBuilders Api Parameters\n",
      "prams = {\n",
      "    'DeveloperKey':'INSERT DEVELOPER KEY',\n",
      "    'Keywords':kw,\n",
      "    'PerPage':50,\n",
      "    'ExcludeKeywords':ekw\n",
      "    }\n",
      "#Create and print search url\n",
      "url = 'http://api.careerbuilder.com/v1/jobsearch?' + urllib.urlencode(prams)\n",
      "#print url\n",
      "\n",
      "#Open connection to search api and return jobs\n",
      "DID_list = []\n",
      "response = urllib2.urlopen(url)\n",
      "xml = response.read()\n",
      "root = ET.fromstring(xml)\n",
      "results = root.find(u'Results')\n",
      "for job in results.findall(u'JobSearchResult'):\n",
      "    DID_list.append(job.find('DID').text)\n",
      "print DID_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['JHR37G606BY23QV98V6', 'JHM2DC71K8NSVTQCY9T', 'J8G0Q864RW60R3SJLRB', 'JHQ63G70RS3XH17FJFR', 'JYR1766C9HH6BN0CT61', 'J3L3MJ6S9TKMGBV749G', 'J3K75P78C3HLFV8JB72', 'J3G0XQ6T9WBPWB2PC89', 'J3L54N6QLBSV1ZMG7FN', 'J3L5256YXDZ0NY3P4BT', 'J3K71168S18FZLKLD5Q', 'J3G1KG6991TZMC9S9BL', 'J3F27K74S5FMBVHHJ7P', 'J3J06T725SPVLCLX7DM', 'J3J0Q66LTR8G9XQBJ0C', 'J3J3HY75MX6ZPWJV80C', 'JHV3TJ5XB6MC68JSL1Z', 'JHV1RK6CRFVGWP97Y0T', 'J3G1HZ67MJG6RJW6TRQ', 'J3H4B367D2412W8D3BB', 'J3F2WM78BT6YF1V5DCX', 'J3K1J96NBNSN3N72NFB', 'J3K4C069M5T3QKYMV69', 'JHQ5ZS6FJNH9QDCWBNS', 'J3F3D86THHQ0SW3C1B4', 'J3F5P86PLWNK88WN1B3', 'J3K0NJ6XYQBJ5M4GNBF', 'J3K5GZ740FXDD3WT6FD', 'J3G3RJ6QXD8CS45P6Q2', 'J3H8GV75S6PW4L27S1Z', 'J3K7VD6L2PTLVMM7RVR', 'J3K15866B9QJ4W7Q717', 'J3K1XG78N12LG6Z6VMS', 'J3H7V66FVVFM8N1Y5M6', 'J3J06T787T7XQZ6BCQ2', 'JHL36475VVJW7WMXVVW', 'J3H4V26N9Y485WMW1TL', 'J3J49J61YM8P563GH2Z', 'J3L8GS5ZVLY24T0M4ZT', 'J3G4YD6S831CNGMF1S4', 'J3H51P6DJG08P758YMW', 'J3K0955YHH59J542QBR', 'J3F80Y6T3WLYB4QDT68', 'JYR07F72WFJ3FKG9FH9', 'JYR2ZJ6ZX0K25JMGBJQ', 'JHL58C5WY0LY9B1NJPL', 'J3K88R631HQ829K6NZW', 'J3G4QQ71X2Q2JP4WYD5', 'J3J0QZ6HKB5NT9KWQR0', 'J3H67C783G2F508M5BX']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Read the Requirements of Each Job"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This is a function that will read each jobID(DID) and return the text of \n",
      "#the requirement section.\n",
      "def get_job(did):\n",
      "    #Define the parameters and create url for api\n",
      "    prams = {\n",
      "        'DeveloperKey':'INSERT DEVELOPER KEY',\n",
      "        'DID':did\n",
      "        }\n",
      "    url = 'http://api.careerbuilder.com/v1/job?' + urllib.urlencode(prams)\n",
      "\n",
      "    #Read the response\n",
      "    response = urllib2.urlopen(url)\n",
      "    xml = response.read()\n",
      "    html_parser = HTMLParser()\n",
      "    #print html_parser.unescape(xml)\n",
      "\n",
      "    #get requirements\n",
      "    root = ET.fromstring(xml)\n",
      "    requirements = strip_tags(html_parser.unescape(root.find(u'Job').find(u'JobRequirements').text))\n",
      "    #fp = open('C:/Users/Aaren/Documents/Projects/%s-requirements.txt'% prams['DID'], 'w')\n",
      "    #fp.write(requirements.decode(errors='ignore'))\n",
      "    #fp.close()\n",
      "    return requirements\n",
      "\n",
      "#This section will place all of the text of the requirment sections for each\n",
      "#job into one large corpus\n",
      "corpus = []\n",
      "for i in range(len(DID_list)):\n",
      "    corpus.append( get_job(DID_list[i]).strip() )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Use sklearn to count the words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Use Sklearn and Numpy to count the occurances of each word in the corpus\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "import numpy\n",
      "countVec = CountVectorizer(min_df=1, ngram_range=(1,2), stop_words='english')\n",
      "x = countVec.fit_transform(corpus)\n",
      "#The goal is to find commonly used words/phrases accross the job descriptions. \n",
      "#Therefore multipule occurances of a word are only counted once to avoid one\n",
      "#job description providing too much weight to any word because the author \n",
      "#really loved to use that word.\n",
      "x[x > 1] = 1\n",
      "count = x.sum(axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sort the list of words/phrases and list the top 100\n",
      "maxNumDisp = 100\n",
      "zipList = zip(countVec.get_feature_names(), list(numpy.array(count).reshape(-1,)) )\n",
      "sortedList = sorted(zipList, key=lambda x:x[1], reverse=True)\n",
      "print sortedList[:maxNumDisp]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'data', 36), (u'python', 31), (u'learning', 25), (u'machine', 24), (u'machine learning', 24), (u'sql', 21), (u'hadoop', 20), (u'mining', 19), (u'data mining', 18), (u'experience', 18), (u'analysis', 17), (u'analytics', 15), (u'skills', 14), (u'statistics', 14), (u'large', 12), (u'sas', 12), (u'science', 12), (u'strong', 12), (u'analytical', 11), (u'business', 11), (u'working', 11), (u'written', 11), (u'complex', 10), (u'degree', 10), (u'job', 10), (u'knowledge', 10), (u'predictive', 10), (u'quantitative', 10), (u'statistical', 10), (u'tools', 10), (u'work', 10), (u'years', 10), (u'ability', 9), (u'advanced', 9), (u'communication', 9), (u'data sets', 9), (u'java', 9), (u'language', 9), (u'modeling', 9), (u'problem', 9), (u'sets', 9), (u'data science', 8), (u'engineering', 8), (u'experience working', 8), (u'mathematics', 8), (u'models', 8), (u'quality', 8), (u'required', 8), (u'requirements', 8), (u'research', 8), (u'team', 8), (u'technical', 8), (u'technology', 8), (u'use', 8), (u'attention', 7), (u'computer', 7), (u'large data', 7), (u'like', 7), (u'management', 7), (u'preferred', 7), (u'problem solving', 7), (u'problems', 7), (u'related', 7), (u'role', 7), (u'skills ability', 7), (u'solving', 7), (u'analytical problem', 6), (u'approaches', 6), (u'big', 6), (u'big data', 6), (u'candidate', 6), (u'click', 6), (u'company', 6), (u'computer science', 6), (u'databases', 6), (u'familiarity', 6), (u'field', 6), (u'high', 6), (u'multiple', 6), (u'operations', 6), (u'oral', 6), (u'ph', 6), (u'predictive analytics', 6), (u'programming', 6), (u'project', 6), (u'proven', 6), (u'relevant', 6), (u'solve', 6), (u'sources', 6), (u'strong written', 6), (u'written oral', 6), (u'abilities', 5), (u'ability work', 5), (u'analyst', 5), (u'available', 5), (u'communicating', 5), (u'communication skills', 5), (u'companies', 5), (u'complex data', 5), (u'contact', 5)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Create a WordCloud from the Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#For use with the wordcloud create a string of the reated phrases where each\n",
      "#word is repeated based on its count. This is to make sure it has the proper \n",
      "#weight in the wordcloud.\n",
      "repeatedPhrases = [' '.join([term[0]]*term[1]) for term in sortedList[:maxNumDisp]]\n",
      "repeatedPhrases = ' '.join(repeatedPhrases)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This section of code came from https://github.com/amueller/word_cloud\n",
      "#Thanks amueller\n",
      "from os import path\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud\n",
      "\n",
      "# Read the whole text.\n",
      "wordcloud = WordCloud().generate(repeatedPhrases)\n",
      "# Open a plot of the generated image.\n",
      "plt.imshow(wordcloud)\n",
      "plt.axis(\"off\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}